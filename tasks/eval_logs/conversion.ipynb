{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OldBench Evaluation Analysis Notebook\n",
    "\n",
    "# %% [markdown]\n",
    "# # Analyze OldBench Evaluation Results\n",
    "# This notebook loads and analyzes your completed evaluation\n",
    "\n",
    "# %% Import libraries\n",
    "from inspect_ai.log import read_eval_log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# %% Load the evaluation log\n",
    "log_file = \"2025-06-14T12-00-29-04-00_oldbench-eval_b5G4V3ymhC27acePkmyWjT.eval\"\n",
    "print(f\"Loading evaluation log: {log_file}\")\n",
    "\n",
    "eval_log = read_eval_log(log_file)\n",
    "print(f\"Evaluation loaded successfully!\")\n",
    "print(f\"Model: {eval_log.eval.model}\")\n",
    "print(f\"Total samples: {eval_log.eval.dataset.samples}\")\n",
    "print(f\"Success rate: {eval_log.eval.results.total_samples}/{eval_log.eval.dataset.samples}\")\n",
    "\n",
    "# %% Convert to DataFrame\n",
    "def eval_to_dataframe(eval_log):\n",
    "    \"\"\"Convert evaluation log to a pandas DataFrame\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for sample in eval_log.samples:\n",
    "        # Extract the question and target answer\n",
    "        question = sample.input\n",
    "        target = sample.target\n",
    "        \n",
    "        # Extract the model's response\n",
    "        model_response = \"\"\n",
    "        for message in sample.messages:\n",
    "            if message.role == \"assistant\" and message.content:\n",
    "                model_response = message.content\n",
    "                break\n",
    "        \n",
    "        # Extract score\n",
    "        score = sample.score.value if sample.score else None\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = sample.metadata or {}\n",
    "        \n",
    "        # Calculate response length\n",
    "        response_length = len(model_response.split())\n",
    "        target_length = len(str(target).split())\n",
    "        \n",
    "        data.append({\n",
    "            'sample_id': sample.id,\n",
    "            'question': question,\n",
    "            'target_answer': target,\n",
    "            'model_response': model_response,\n",
    "            'score': score,\n",
    "            'directorate': metadata.get('directorate', 'unknown'),\n",
    "            'question_type': metadata.get('question_type', 'unknown'),\n",
    "            'response_length': response_length,\n",
    "            'target_length': target_length,\n",
    "            'error': sample.error.message if sample.error else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = eval_to_dataframe(eval_log)\n",
    "print(f\"\\nCreated DataFrame with {len(df)} samples\")\n",
    "\n",
    "# %% Display overall statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Successful: {df['score'].notna().sum()}\")\n",
    "print(f\"Failed: {df['score'].isna().sum()}\")\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(f\"Mean Score: {df['score'].mean():.3f}\")\n",
    "print(f\"Median Score: {df['score'].median():.3f}\")\n",
    "print(f\"Std Dev: {df['score'].std():.3f}\")\n",
    "print(f\"Min Score: {df['score'].min():.3f}\")\n",
    "print(f\"Max Score: {df['score'].max():.3f}\")\n",
    "\n",
    "# %% Score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['score'].hist(bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Evaluation Scores')\n",
    "plt.axvline(df['score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"score\"].mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# %% Analyze by directorate\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS BY DIRECTORATE\")\n",
    "print(\"=\" * 60)\n",
    "directorate_stats = df.groupby('directorate').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'response_length': 'mean'\n",
    "}).round(3)\n",
    "directorate_stats.columns = ['Count', 'Mean Score', 'Std Dev', 'Min Score', 'Max Score', 'Avg Response Length']\n",
    "directorate_stats = directorate_stats.sort_values('Mean Score', ascending=False)\n",
    "display(directorate_stats)\n",
    "\n",
    "# %% Visualize by directorate\n",
    "plt.figure(figsize=(12, 6))\n",
    "directorate_means = df.groupby('directorate')['score'].mean().sort_values(ascending=True)\n",
    "plt.barh(directorate_means.index, directorate_means.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.title('Mean Scores by Directorate')\n",
    "plt.xlim(0, 1)\n",
    "for i, v in enumerate(directorate_means.values):\n",
    "    plt.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% Analyze by question type\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS BY QUESTION TYPE\")\n",
    "print(\"=\" * 60)\n",
    "question_type_stats = df.groupby('question_type').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'response_length': 'mean'\n",
    "}).round(3)\n",
    "question_type_stats.columns = ['Count', 'Mean Score', 'Std Dev', 'Min Score', 'Max Score', 'Avg Response Length']\n",
    "question_type_stats = question_type_stats.sort_values('Mean Score', ascending=False)\n",
    "display(question_type_stats)\n",
    "\n",
    "# %% Find best and worst performing questions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 5 BEST PERFORMING QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "best_questions = df.nlargest(5, 'score')[['question', 'score', 'directorate', 'question_type']]\n",
    "for idx, row in best_questions.iterrows():\n",
    "    print(f\"\\nScore: {row['score']:.3f} | {row['directorate']} | {row['question_type']}\")\n",
    "    print(f\"Q: {row['question'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 5 WORST PERFORMING QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "worst_questions = df.nsmallest(5, 'score')[['question', 'score', 'directorate', 'question_type']]\n",
    "for idx, row in worst_questions.iterrows():\n",
    "    print(f\"\\nScore: {row['score']:.3f} | {row['directorate']} | {row['question_type']}\")\n",
    "    print(f\"Q: {row['question'][:150]}...\")\n",
    "\n",
    "# %% Response length analysis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['target_length'], df['score'], alpha=0.6)\n",
    "plt.xlabel('Target Answer Length (words)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score vs Target Answer Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['response_length'], df['score'], alpha=0.6)\n",
    "plt.xlabel('Model Response Length (words)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score vs Model Response Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% Create a detailed results table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RESULTS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "results_table = df[['question', 'score', 'directorate', 'question_type']].copy()\n",
    "results_table['question'] = results_table['question'].str[:80] + '...'\n",
    "results_table = results_table.sort_values('score', ascending=False)\n",
    "display(results_table.head(10))\n",
    "\n",
    "# %% Export to CSV for further analysis\n",
    "output_filename = \"oldbench_evaluation_results.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults exported to: {output_filename}\")\n",
    "\n",
    "# %% Create a summary report\n",
    "summary = {\n",
    "    'total_samples': len(df),\n",
    "    'mean_score': df['score'].mean(),\n",
    "    'std_score': df['score'].std(),\n",
    "    'best_directorate': directorate_stats.index[0],\n",
    "    'worst_directorate': directorate_stats.index[-1],\n",
    "    'best_question_type': question_type_stats.index[0],\n",
    "    'worst_question_type': question_type_stats.index[-1],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e95a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/homebrew/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/homebrew/lib/python3.11/site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/homebrew/lib/python3.11/site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tanavthanjavuru/Library/Python/3.11/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tanavthanjavuru/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fd368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
